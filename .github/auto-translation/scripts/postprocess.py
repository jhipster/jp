#!/usr/bin/env python3
"""
JHipsteræ—¥æœ¬èªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè‡ªå‹•ç¿»è¨³ã‚·ã‚¹ãƒ†ãƒ 
ãƒã‚¹ãƒˆãƒ—ãƒ­ã‚»ãƒƒã‚·ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import argparse
import json
import os
import re
import sys
from pathlib import Path
from typing import List, Optional


def find_project_root() -> Path:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¦‹ã¤ã‘ã‚‹"""
    current = Path(__file__).parent
    while current != current.parent:
        if (current / '.git').exists() or (current / 'package.json').exists():
            return current
        current = current.parent
    return Path.cwd()

try:
    import language_tool_python
    LANGUAGE_TOOL_AVAILABLE = True
except ImportError:
    LANGUAGE_TOOL_AVAILABLE = False
    print("âš ï¸ language-tool-python not available, skipping grammar check")


class PostProcessor:
    def __init__(self, enable_language_tool: bool = True):
        self.enable_language_tool = enable_language_tool and LANGUAGE_TOOL_AVAILABLE
        self.language_tool = None
        self.project_root = find_project_root()
        
        if self.enable_language_tool:
            try:
                self.language_tool = language_tool_python.LanguageTool('ja')
                print("âœ… LanguageTool initialized for Japanese")
            except Exception as e:
                print(f"âš ï¸ Failed to initialize LanguageTool: {e}")
                self.enable_language_tool = False
    
    def remove_conflict_markers(self, content: str) -> str:
        """Git ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆãƒãƒ¼ã‚«ãƒ¼ã‚’é™¤å»"""
        # ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆãƒãƒ¼ã‚«ãƒ¼ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns = [
            r'^<{4,7} .*\n',  # <<<<<<< HEAD
            r'^={4,7}\n',     # =======
            r'^>{4,7} .*\n',  # >>>>>>> branch
        ]
        
        lines = content.split('\n')
        cleaned_lines = []
        
        for line in lines:
            is_conflict_marker = False
            for pattern in patterns:
                if re.match(pattern, line + '\n', re.MULTILINE):
                    is_conflict_marker = True
                    break
            
            if not is_conflict_marker:
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def check_line_count_consistency(self, original_file: str, processed_file: str) -> bool:
        """åŸæ–‡ã¨ç¿»è¨³ã®è¡Œæ•°ä¸€è‡´ã‚’ç¢ºèª"""
        try:
            original_path = self.project_root / original_file
            processed_path = self.project_root / processed_file
            
            with open(original_path, 'r', encoding='utf-8') as f:
                original_lines = len(f.readlines())
            
            with open(processed_path, 'r', encoding='utf-8') as f:
                processed_lines = len(f.readlines())
            
            if original_lines != processed_lines:
                print(f"âš ï¸ Line count mismatch: {original_file}({original_lines}) vs {processed_file}({processed_lines})")
                return False
            
            return True
            
        except Exception as e:
            print(f"âŒ Error checking line count: {e}")
            return False
    
    def check_grammar(self, text: str) -> List[dict]:
        """æ–‡æ³•ãƒã‚§ãƒƒã‚¯ï¼ˆLanguageToolã‚’ä½¿ç”¨ï¼‰"""
        if not self.enable_language_tool or not self.language_tool:
            return []
        
        try:
            matches = self.language_tool.check(text)
            return [
                {
                    "message": match.message,
                    "offset": match.offset,
                    "length": match.length,
                    "context": match.context,
                    "suggestions": [r for r in match.replacements[:3]]  # æœ€åˆã®3ã¤ã®ææ¡ˆã®ã¿
                }
                for match in matches
            ]
        except Exception as e:
            print(f"âš ï¸ Grammar check error: {e}")
            return []
    
    def validate_markdown_structure(self, content: str) -> List[str]:
        """ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³æ§‹é€ ã®æ¤œè¨¼"""
        issues = []
        lines = content.split('\n')
        
        # åŸºæœ¬çš„ãªæ¤œè¨¼
        for i, line in enumerate(lines, 1):
            # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®å¯¾ç§°æ€§ãƒã‚§ãƒƒã‚¯
            if line.strip().startswith('```'):
                # ã“ã®å®Ÿè£…ã¯ç°¡å˜åŒ–ã•ã‚Œã¦ã„ã¾ã™
                pass
            
            # ãƒªãƒ³ã‚¯ã®å½¢å¼ãƒã‚§ãƒƒã‚¯
            if '[' in line and ']' in line:
                # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒªãƒ³ã‚¯ã®åŸºæœ¬çš„ãªå½¢å¼ãƒã‚§ãƒƒã‚¯
                link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
                links = re.findall(link_pattern, line)
                for text, url in links:
                    if not url or url.isspace():
                        issues.append(f"Line {i}: Empty link URL")
        
        return issues
    
    def process_file(self, file_path: str, output_path: Optional[str] = None) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¹ãƒˆãƒ—ãƒ­ã‚»ãƒƒã‚·ãƒ³ã‚°"""
        try:
            full_file_path = self.project_root / file_path
            with open(full_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            print(f"ğŸ“ Post-processing: {file_path}")
            
            # ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆãƒãƒ¼ã‚«ãƒ¼ã‚’é™¤å»
            original_content = content
            content = self.remove_conflict_markers(content)
            
            if content != original_content:
                print("   âœ… Removed conflict markers")
            
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³æ§‹é€ æ¤œè¨¼
            markdown_issues = self.validate_markdown_structure(content)
            if markdown_issues:
                print("   âš ï¸ Markdown structure issues:")
                for issue in markdown_issues[:5]:  # æœ€åˆã®5ã¤ã®ã¿è¡¨ç¤º
                    print(f"      - {issue}")
            
            # æ–‡æ³•ãƒã‚§ãƒƒã‚¯
            if self.enable_language_tool:
                # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯åˆ†å‰²ã—ã¦ãƒã‚§ãƒƒã‚¯
                max_chunk_size = 5000
                if len(content) > max_chunk_size:
                    print("   ğŸ“ Running grammar check (in chunks)...")
                    chunks = [content[i:i+max_chunk_size] for i in range(0, len(content), max_chunk_size)]
                    all_grammar_issues = []
                    for chunk in chunks:
                        chunk_issues = self.check_grammar(chunk)
                        all_grammar_issues.extend(chunk_issues)
                else:
                    print("   ğŸ“ Running grammar check...")
                    all_grammar_issues = self.check_grammar(content)
                
                if all_grammar_issues:
                    print(f"   âš ï¸ Found {len(all_grammar_issues)} potential grammar issues")
                    for issue in all_grammar_issues[:3]:  # æœ€åˆã®3ã¤ã®ã¿è¡¨ç¤º
                        print(f"      - {issue['message']}")
                        if issue['suggestions']:
                            print(f"        Suggestions: {', '.join(issue['suggestions'])}")
                else:
                    print("   âœ… No grammar issues found")
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ±ºå®š
            if output_path is None:
                full_output_path = full_file_path
            else:
                full_output_path = self.project_root / output_path
            
            # å‡¦ç†çµæœã‚’ä¿å­˜
            full_output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(full_output_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print(f"âœ… Post-processing completed: {full_output_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Error post-processing file {file_path}: {e}")
            return False
    
    def process_files_from_classification(self, classification_file: str) -> bool:
        """åˆ†é¡çµæœã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¹ãƒˆãƒ—ãƒ­ã‚»ãƒƒã‚·ãƒ³ã‚°"""
        try:
            full_classification_path = self.project_root / classification_file
            with open(full_classification_path, 'r', encoding='utf-8') as f:
                classification = json.load(f)
            
            # ç¿»è¨³æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¯¾è±¡ã¨ã™ã‚‹
            files_to_process = []
            files_to_process.extend(classification["summary"]["a"])
            files_to_process.extend(classification["summary"]["b-1"])
            files_to_process.extend(classification["summary"]["b-2"])
            
            if not files_to_process:
                print("ğŸ“‹ No files to post-process")
                return True
            
            print(f"ğŸ“‹ Found {len(files_to_process)} files to post-process")
            
            success_count = 0
            for file_path in files_to_process:
                full_file_path = self.project_root / file_path
                if full_file_path.exists():
                    if self.process_file(file_path):
                        success_count += 1
                    else:
                        print(f"âŒ Failed to post-process: {file_path}")
                else:
                    print(f"âš ï¸ File not found: {file_path}")
            
            print(f"âœ… Post-processing completed: {success_count}/{len(files_to_process)} files")
            return success_count == len(files_to_process)
            
        except Exception as e:
            print(f"âŒ Error in batch post-processing: {e}")
            return False
    
    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.language_tool:
            self.language_tool.close()


def main():
    parser = argparse.ArgumentParser(description="Post-process translated files")
    parser.add_argument(
        "--file",
        help="Single file to post-process"
    )
    parser.add_argument(
        "--classification",
        help="Classification JSON file for batch post-processing"
    )
    parser.add_argument(
        "--output",
        help="Output file path (for single file processing)"
    )
    parser.add_argument(
        "--no-grammar-check",
        action="store_true",
        help="Disable grammar checking"
    )
    
    args = parser.parse_args()
    
    # LanguageToolè¨­å®š
    enable_language_tool = not args.no_grammar_check
    if os.getenv("LANGUAGE_TOOL_ENABLED", "true").lower() == "false":
        enable_language_tool = False
    
    processor = PostProcessor(enable_language_tool)
    
    try:
        if args.file:
            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
            success = processor.process_file(args.file, args.output)
            sys.exit(0 if success else 1)
        elif args.classification:
            # ãƒãƒƒãƒå‡¦ç†
            success = processor.process_files_from_classification(args.classification)
            sys.exit(0 if success else 1)
        else:
            print("âŒ Either --file or --classification must be specified")
            sys.exit(1)
    finally:
        processor.cleanup()


if __name__ == "__main__":
    main()